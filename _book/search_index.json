[
["lab-3-regression-for-prediction.html", "Chapter 3 Lab 3: Regression for prediction 3.1 Introduction 3.2 Linear Regression", " Chapter 3 Lab 3: Regression for prediction 3.1 Introduction In this lab we are going to focus on predicting house prices with the tools described in the chapter. The interest in predicting house prices is not new, but it’s has proven to be a quite challenging problem where machine learning may have some interesting input. For example the data science competitions company is hosting a competition desined to improve estimates with a prize of one million dollars. We consider the data set included in the for . The data includes sales prices, structural characteristics and geo-location of single-family homes on the Far North Side of Chicago sold in 1995 and 2005. We randomly divided the sample into a trianing and a testing sample. load(&quot;house_data.rda&quot;) ls() ## [1] &quot;test&quot; &quot;train&quot; stargazer::stargazer(train, header=FALSE, type=&#39;latex&#39;) ## ## \\begin{table}[!htbp] \\centering ## \\caption{} ## \\label{} ## \\begin{tabular}{@{\\extracolsep{5pt}}lccccccc} ## \\\\[-1.8ex]\\hline ## \\hline \\\\[-1.8ex] ## Statistic &amp; \\multicolumn{1}{c}{N} &amp; \\multicolumn{1}{c}{Mean} &amp; \\multicolumn{1}{c}{St. Dev.} &amp; \\multicolumn{1}{c}{Min} &amp; \\multicolumn{1}{c}{Pctl(25)} &amp; \\multicolumn{1}{c}{Pctl(75)} &amp; \\multicolumn{1}{c}{Max} \\\\ ## \\hline \\\\[-1.8ex] ## year &amp; 2,704 &amp; 1,999.963 &amp; 5.001 &amp; 1,995 &amp; 1,995 &amp; 2,005 &amp; 2,005 \\\\ ## lnland &amp; 2,704 &amp; 8.305 &amp; 0.386 &amp; 6.633 &amp; 8.221 &amp; 8.509 &amp; 10.076 \\\\ ## lnbldg &amp; 2,704 &amp; 7.178 &amp; 0.287 &amp; 6.174 &amp; 6.985 &amp; 7.364 &amp; 8.356 \\\\ ## rooms &amp; 2,704 &amp; 5.780 &amp; 1.229 &amp; 2 &amp; 5 &amp; 6 &amp; 12 \\\\ ## bedrooms &amp; 2,704 &amp; 3.017 &amp; 0.744 &amp; 1 &amp; 3 &amp; 3 &amp; 7 \\\\ ## bathrooms &amp; 2,704 &amp; 1.418 &amp; 0.514 &amp; 1.000 &amp; 1.000 &amp; 1.500 &amp; 5.000 \\\\ ## centair &amp; 2,704 &amp; 0.350 &amp; 0.477 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ ## fireplace &amp; 2,704 &amp; 0.159 &amp; 0.365 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ ## brick &amp; 2,704 &amp; 0.670 &amp; 0.470 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ ## garage1 &amp; 2,704 &amp; 0.308 &amp; 0.462 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ ## garage2 &amp; 2,704 &amp; 0.449 &amp; 0.498 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ ## dcbd &amp; 2,704 &amp; 9.697 &amp; 1.719 &amp; 5.245 &amp; 8.459 &amp; 10.975 &amp; 13.587 \\\\ ## rr &amp; 2,704 &amp; 0.161 &amp; 0.368 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ ## yrbuilt &amp; 2,704 &amp; 1,935.040 &amp; 21.704 &amp; 1,876 &amp; 1,919.8 &amp; 1,952 &amp; 1,991 \\\\ ## latitude &amp; 2,704 &amp; 41.987 &amp; 0.015 &amp; 41.956 &amp; 41.975 &amp; 41.997 &amp; 42.022 \\\\ ## longitude &amp; 2,704 &amp; $-$87.745 &amp; 0.050 &amp; $-$87.834 &amp; $-$87.790 &amp; $-$87.699 &amp; $-$87.647 \\\\ ## lnprice &amp; 2,704 &amp; 12.405 &amp; 0.524 &amp; 10.166 &amp; 11.951 &amp; 12.835 &amp; 13.825 \\\\ ## age &amp; 2,704 &amp; 64.923 &amp; 22.325 &amp; 4 &amp; 48 &amp; 82 &amp; 129 \\\\ ## \\hline \\\\[-1.8ex] ## \\end{tabular} ## \\end{table} 3.2 Linear Regression 3.2.1 MSE and regression The objective then is to be able to get the best prediction of house prices. We begin by using a simple model with no covariates, just a constant model1&lt;-lm(lnprice~1,data=train) summary(model1) ## ## Call: ## lm(formula = lnprice ~ 1, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.23948 -0.45415 0.01787 0.42935 1.42013 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.40533 0.01008 1230 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5244 on 2703 degrees of freedom In this case our prediction for the log price is the average train sample average \\[ \\hat{y}=\\hat{\\beta_1}=\\frac{\\sum y_i}{n}=m \\] coef(model1) ## (Intercept) ## 12.40533 mean(train$lnprice) ## [1] 12.40533 But we are concernded on predicting well our of sample, so we need to evaluate our model in the testing data test$model1&lt;-predict(model1,newdata = test) with(test,mean((lnprice-model1)^2)) ## [1] 0.2857339 Then the \\(MSE=E(y-\\hat{y})=E(y-m)=\\). This is our starting point, then the question is how can we improve it. 3.2.2 Complexity To improve our prediction we can start adding variables and thus \\(f\\). The standard approach to build \\(f\\) would be using a hedonic house price function derived directly from the theory of hedonic pricing ((???)). In its basic form the hedonic price function is linear in the explanatory characteristics \\[ y=\\beta_1+\\beta_2 x_2 + \\dots + \\beta_K x_k +u \\] where \\(y\\) is ussually the log of the sale price, and \\(x_1 \\dots x_k\\) are attributes of the house, like structural characteristics and it’s location. So estimating an hedonic price function seems a good idea to start with. However, the theory says little on what are the relevant attributes of the house. So we are going to procede with one foot in the theory and one foot in the data, to guide us in building \\(f\\). We begin by showing that the simple inclusion of a single covariate reduces the MSE with respect to the model that used the sample mean. model2&lt;-lm(lnprice~bedrooms,data=train) test$model2&lt;-predict(model2,newdata = test) with(test,mean((lnprice-model2)^2)) ## [1] 0.2836981 What about if we include more variables? model3&lt;-lm(lnprice~bedrooms+bathrooms+centair+fireplace+brick+age,data=train) test$model3&lt;-predict(model3,newdata = test) with(test,mean((lnprice-model3)^2)) ## [1] 0.255731 model4&lt;-lm(lnprice~bedrooms+bathrooms+centair+fireplace+brick+poly(age,2),data=train) test$model4&lt;-predict(model4,newdata = test) Note that the MSE is once more reduced. What about if we include some non linear variables, like \\(age\\) and \\(age^2\\)?. Then the MSE for model 3 goes from to . In this case the MSE gets slightly worse, showing how we are subject to the bias/variance trade off. 3.2.3 Non-linear models In practice empirical researches have recognized that hedonic house functions are likely to be nonlinear in structural characteristics, specially when it comes to continuous measures of location variables such as distance from the city center (???). Idea Show examples of other models It is clear that we have a wide pallete of models to choose from. But the question is how do we choose from. And this is where supervised machine learning can give some guidance. 3.2.4 Comparison on the models "]
]
