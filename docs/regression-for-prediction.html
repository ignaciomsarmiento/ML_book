<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lab 3 Regression for prediction | Statistical and Machine Learning for Economists and Social Scientists</title>
  <meta name="description" content="This book contains the labs for Statistical and Machine Learning for Economists and Social Scientists" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Lab 3 Regression for prediction | Statistical and Machine Learning for Economists and Social Scientists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ignaciomsarmiento.github.io/ML_book_codes/" />
  
  <meta property="og:description" content="This book contains the labs for Statistical and Machine Learning for Economists and Social Scientists" />
  <meta name="github-repo" content="ignaciomsarmiento/ML_book_codes-start" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lab 3 Regression for prediction | Statistical and Machine Learning for Economists and Social Scientists" />
  
  <meta name="twitter:description" content="This book contains the labs for Statistical and Machine Learning for Economists and Social Scientists" />
  

<meta name="author" content="Ignacio Sarmiento Barbieri" />


<meta name="date" content="2020-06-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lab-2-regression-from-a-classical-perspective.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical and Machine Learning for Economists and Social Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Lab 1: Introduction to R</a></li>
<li class="chapter" data-level="2" data-path="lab-2-regression-from-a-classical-perspective.html"><a href="lab-2-regression-from-a-classical-perspective.html"><i class="fa fa-check"></i><b>2</b> Lab 2: Regression from a classical perspective</a></li>
<li class="chapter" data-level="3" data-path="regression-for-prediction.html"><a href="regression-for-prediction.html"><i class="fa fa-check"></i><b>3</b> Regression for prediction</a><ul>
<li class="chapter" data-level="3.1" data-path="regression-for-prediction.html"><a href="regression-for-prediction.html#sec:introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="regression-for-prediction.html"><a href="regression-for-prediction.html#linear-regression"><i class="fa fa-check"></i><b>3.2</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="regression-for-prediction.html"><a href="regression-for-prediction.html#mse-and-regression"><i class="fa fa-check"></i><b>3.2.1</b> MSE and regression</a></li>
<li class="chapter" data-level="3.2.2" data-path="regression-for-prediction.html"><a href="regression-for-prediction.html#complexity"><i class="fa fa-check"></i><b>3.2.2</b> Complexity</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regression-for-prediction.html"><a href="regression-for-prediction.html#goodness-of-fit.-in-and-out-of-sample-performance"><i class="fa fa-check"></i><b>3.3</b> Goodness-of-fit. In and out of sample performance</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical and Machine Learning for Economists and Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-for-prediction" class="section level1">
<h1><span class="header-section-number">Lab  3</span> Regression for prediction</h1>
<div id="sec:introduction" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>In this lab we are going to focus on predicting house prices with the tools described in the chapter. The interest in predicting house prices is not new, but it’s has proven to be a quite challenging problem where machine learning may have some interesting input. For example the data science competitions company  is hosting a competition desined to improve  estimates with a prize of one million dollars.</p>
<p>We consider the data set included in the <em>McSpatial package</em> <span class="citation">(McMillen <a href="#ref-mcmspatial">2013</a>)</span> for R. The data includes sales prices, structural characteristics and geo-location of single-family homes on the Far North Side of Chicago sold in 1995 and 2005. We randomly divided the sample into a trianing and a testing sample.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">load</span>(<span class="st">&quot;house_data.rda&quot;</span>)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">ls</span>()</a></code></pre></div>
<pre><code>## [1] &quot;test&quot;  &quot;train&quot;</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">stargazer<span class="op">::</span><span class="kw">stargazer</span>(train, <span class="dt">header=</span><span class="ot">FALSE</span>, <span class="dt">type=</span><span class="st">&#39;text&#39;</span>)</a></code></pre></div>
<pre><code>## 
## ====================================================================
## Statistic   N     Mean    St. Dev.   Min   Pctl(25) Pctl(75)   Max  
## --------------------------------------------------------------------
## year      2,704 1,999.963  5.001    1,995   1,995    2,005    2,005 
## lnland    2,704   8.305    0.386    6.633   8.221    8.509   10.076 
## lnbldg    2,704   7.178    0.287    6.174   6.985    7.364    8.356 
## rooms     2,704   5.780    1.229      2       5        6       12   
## bedrooms  2,704   3.017    0.744      1       3        3        7   
## bathrooms 2,704   1.418    0.514    1.000   1.000    1.500    5.000 
## centair   2,704   0.350    0.477      0       0        1        1   
## fireplace 2,704   0.159    0.365      0       0        0        1   
## brick     2,704   0.670    0.470      0       0        1        1   
## garage1   2,704   0.308    0.462      0       0        1        1   
## garage2   2,704   0.449    0.498      0       0        1        1   
## dcbd      2,704   9.697    1.719    5.245   8.459    10.975  13.587 
## rr        2,704   0.161    0.368      0       0        0        1   
## yrbuilt   2,704 1,935.040  21.704   1,876  1,919.8   1,952    1,991 
## latitude  2,704  41.987    0.015   41.956   41.975   41.997  42.022 
## longitude 2,704  -87.745   0.050   -87.834 -87.790  -87.699  -87.647
## lnprice   2,704  12.405    0.524   10.166   11.951   12.835  13.825 
## age       2,704  64.923    22.325     4       48       82      129  
## --------------------------------------------------------------------</code></pre>
</div>
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Linear Regression</h2>
<div id="mse-and-regression" class="section level3">
<h3><span class="header-section-number">3.2.1</span> MSE and regression</h3>
<p>The objective then is to be able to get the best prediction of house prices. We begin by using a simple model with no covariates, just a constant</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">model1&lt;-<span class="kw">lm</span>(lnprice<span class="op">~</span><span class="dv">1</span>,<span class="dt">data=</span>train)</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">summary</span>(model1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = lnprice ~ 1, data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.23948 -0.45415  0.01787  0.42935  1.42013 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 12.40533    0.01008    1230   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5244 on 2703 degrees of freedom</code></pre>
<p>In this case our prediction for the log price is the average train sample average</p>
<p><span class="math display">\[
\hat{y}=\hat{\beta_1}=\frac{\sum y_i}{n}=m
\]</span></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="kw">coef</span>(model1)</a></code></pre></div>
<pre><code>## (Intercept) 
##    12.40533</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">mean</span>(train<span class="op">$</span>lnprice)</a></code></pre></div>
<pre><code>## [1] 12.40533</code></pre>
<p>But we are concernded on predicting well our of sample, so we need to evaluate our model in the testing data</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">test<span class="op">$</span>model1&lt;-<span class="kw">predict</span>(model1,<span class="dt">newdata =</span> test)</a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="kw">with</span>(test,<span class="kw">mean</span>((lnprice<span class="op">-</span>model1)<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 0.2857339</code></pre>
<p>Then the <span class="math inline">\(MSE=E(y-\hat{y})=E(y-m)=\)</span> 0.2857339. This is our starting point, then the question is how can we improve it.</p>
</div>
<div id="complexity" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Complexity</h3>
<p>To improve our prediction we can start adding variables and thus <em>building</em> <span class="math inline">\(f\)</span>. The standard approach to build <span class="math inline">\(f\)</span> would be using a hedonic house price function derived directly from the theory of hedonic pricing <span class="citation">(Rosen <a href="#ref-rosen1974hedonic">1974</a>)</span>. In its basic form the hedonic price function is linear in the explanatory characteristics</p>
<p><span class="math display">\[
y=\beta_1+\beta_2 x_2 + \dots + \beta_K x_k +u
 \]</span></p>
<p>where <span class="math inline">\(y\)</span> is ussually the log of the sale price, and <span class="math inline">\(x_1 \dots x_k\)</span> are attributes of the house, like structural characteristics and it’s location. So estimating an hedonic price function seems a good idea to start with.
However, the theory says little on what are the relevant attributes of the house. So we are going to procede with one foot in the theory and one foot in the data, to guide us in building <span class="math inline">\(f\)</span>.</p>
<p>We begin by showing that the simple inclusion of a single covariate reduces the MSE with respect to the  model that used the sample mean.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">model2&lt;-<span class="kw">lm</span>(lnprice<span class="op">~</span>bedrooms,<span class="dt">data=</span>train)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">test<span class="op">$</span>model2&lt;-<span class="kw">predict</span>(model2,<span class="dt">newdata =</span> test)</a>
<a class="sourceLine" id="cb13-3" data-line-number="3"><span class="kw">with</span>(test,<span class="kw">mean</span>((lnprice<span class="op">-</span>model2)<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 0.2836981</code></pre>
<p>What about if we include more variables?</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">model3&lt;-<span class="kw">lm</span>(lnprice<span class="op">~</span>bedrooms<span class="op">+</span>bathrooms<span class="op">+</span>centair<span class="op">+</span>fireplace<span class="op">+</span>brick<span class="op">+</span>age,<span class="dt">data=</span>train)</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">test<span class="op">$</span>model3&lt;-<span class="kw">predict</span>(model3,<span class="dt">newdata =</span> test)</a>
<a class="sourceLine" id="cb15-3" data-line-number="3"><span class="kw">with</span>(test,<span class="kw">mean</span>((lnprice<span class="op">-</span>model3)<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 0.255731</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">model4&lt;-<span class="kw">lm</span>(lnprice<span class="op">~</span>bedrooms<span class="op">+</span>bathrooms<span class="op">+</span>centair<span class="op">+</span>fireplace<span class="op">+</span>brick<span class="op">+</span><span class="kw">poly</span>(age,<span class="dv">2</span>),<span class="dt">data=</span>train)</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">test<span class="op">$</span>model4&lt;-<span class="kw">predict</span>(model4,<span class="dt">newdata =</span> test)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3"><span class="kw">with</span>(test,<span class="kw">mean</span>((lnprice<span class="op">-</span>model4)<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 0.2565857</code></pre>
<p>Note that the MSE is once more reduced. What about if we include some non linear variables, like <span class="math inline">\(age\)</span> and <span class="math inline">\(age^2\)</span>?. Then the MSE for model 3 goes from 0.255731 to 0.2565857. In this case the MSE gets slightly worse, showing how we are subject to the bias/variance trade off.</p>
</div>
</div>
<div id="goodness-of-fit.-in-and-out-of-sample-performance" class="section level2">
<h2><span class="header-section-number">3.3</span> Goodness-of-fit. In and out of sample performance</h2>
<p>As stated before the mechanics of OLS give rise to a simple measure of <em>goodness of fit</em>: <span class="math inline">\(R^2\)</span>. The <span class="math inline">\(R^2\)</span> takes values between 0 and 1, where 1 implies that the model <em>“fits”</em> the data <em>“perfectly”</em> and 0 is the opposite. This is a statistic that is ussually reported with regression results, but can also be easily calculated. By invoking the summary function on our estimated model 2 it reports the <em>Multiple R-squared</em></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">summary</span>(model2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = lnprice ~ bedrooms, data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.23830 -0.45220  0.02506  0.43053  1.42131 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 12.20177    0.04195 290.886  &lt; 2e-16 ***
## bedrooms     0.06746    0.01350   4.998 6.16e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5221 on 2702 degrees of freedom
## Multiple R-squared:  0.009161,   Adjusted R-squared:  0.008794 
## F-statistic: 24.98 on 1 and 2702 DF,  p-value: 6.16e-07</code></pre>
<p>And we can obtain it by calling:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="kw">summary</span>(model2)<span class="op">$</span>r.squared</a></code></pre></div>
<pre><code>## [1] 0.009160653</code></pre>
<p>But we can also calcuate it <em>“by hand”</em>. We leverage the <em>sum of squares decomposition</em></p>
<p><span class="math display">\[\begin{align}
 TSS &amp;= \sum y_i^2   \\
 ESS &amp;= \sum \hat y_i^2 \\
 RSS &amp;=\sum e_i^2 \\
\end{align}\]</span></p>
<p>where <span class="math inline">\(y_i \equiv Y_i - \bar Y\)</span>, <span class="math inline">\(\hat y_i \equiv \hat Y_i - \bar Y\)</span> and <span class="math inline">\(e_i\)</span> are OLS residuals.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">Ybar&lt;-<span class="kw">mean</span>(train<span class="op">$</span>lnprice)</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">Y_i&lt;-train<span class="op">$</span>lnprice</a>
<a class="sourceLine" id="cb23-3" data-line-number="3">Y_hat&lt;-<span class="kw">predict</span>(model2,<span class="dt">newdata =</span> train)</a>
<a class="sourceLine" id="cb23-4" data-line-number="4">e_i&lt;-model2<span class="op">$</span>residuals</a></code></pre></div>
<p>We have now all the <em>ingredients</em></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">TSS&lt;-<span class="st"> </span><span class="kw">sum</span>((Y_i<span class="op">-</span>Ybar)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb24-2" data-line-number="2">ESS&lt;-<span class="st"> </span><span class="kw">sum</span>((Y_hat<span class="op">-</span>Ybar)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb24-3" data-line-number="3">RSS&lt;-<span class="st"> </span><span class="kw">sum</span>(e_i<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb24-4" data-line-number="4">R2&lt;-<span class="st"> </span>ESS<span class="op">/</span>TSS</a>
<a class="sourceLine" id="cb24-5" data-line-number="5">R2</a></code></pre></div>
<pre><code>## [1] 0.009160653</code></pre>
<p>Or another way</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1">R2&lt;-<span class="st"> </span><span class="dv">1</span><span class="op">-</span>(RSS<span class="op">/</span>TSS)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">R2</a></code></pre></div>
<pre><code>## [1] 0.009160653</code></pre>
<p>note that these approaches give the same result</p>

<div id="refs" class="references">
<div>
<p>McMillen, Daniel. 2013. <em>McSpatial: Nonparametric Spatial Data Analysis</em>. <a href="https://CRAN.R-project.org/package=McSpatial">https://CRAN.R-project.org/package=McSpatial</a>.</p>
</div>
<div>
<p>Rosen, Sherwin. 1974. “Hedonic Prices and Implicit Markets: Product Differentiation in Pure Competition.” <em>Journal of Political Economy</em> 82 (1). The University of Chicago Press: 34–55.</p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-mcmspatial">
<p>McMillen, Daniel. 2013. <em>McSpatial: Nonparametric Spatial Data Analysis</em>. <a href="https://CRAN.R-project.org/package=McSpatial">https://CRAN.R-project.org/package=McSpatial</a>.</p>
</div>
<div id="ref-rosen1974hedonic">
<p>Rosen, Sherwin. 1974. “Hedonic Prices and Implicit Markets: Product Differentiation in Pure Competition.” <em>Journal of Political Economy</em> 82 (1). The University of Chicago Press: 34–55.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lab-2-regression-from-a-classical-perspective.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
